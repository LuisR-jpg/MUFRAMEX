{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26120acd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In most of this school, we will be coding with the **Pytorch library**.\n",
    "\n",
    "It is rather simple to use, and is centred around three main concepts:\n",
    "\n",
    "* **Tensor** as a central data structure.\n",
    "* **Auto-differenciation** for implementing the **computation of gradients** in a transparent way for the programmer. \n",
    "* Library of state-of-the-art **building blocks** to assemble neural networks. \n",
    "\n",
    "(All the following notebook is inspired from the Pytorch documentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a154748a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The tensor type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9246725",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The **tensor** data structure is the central one in most DL libraries, and in particular in Pytorch.\n",
    "\n",
    "Corresponds to the mathematical concept of **tensor** (generalization of matrices, in an arbitrary number of dimensions). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fda106",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b71e0a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The same concept can be implemented:\n",
    "\n",
    "* As a **Python multiple array**.\n",
    "* As a **Numpy array**.\n",
    "* As a Pytorch **tensor**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c6555b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "python_array  = [[[1, 2], [3, 4]],[[5, 6], [7, 8]]]\n",
    "numpy_array   = np.array(python_array)\n",
    "torch_tensor  = torch.from_numpy(numpy_array)\n",
    "print(python_array)\n",
    "print(numpy_array)\n",
    "print(torch_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf722624",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are **many ways to build/initialize a tensor**:\n",
    "\n",
    "* from scratch,\n",
    "* from a numpy array,\n",
    "* as a zero/ones tensor of specified shape,\n",
    "* as a zero/ones tensor with the characteristics (size) of another tensor,\n",
    "* from a random number generator,\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91315e6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "shape              = (2, 3, 2)\n",
    "torch_tensor_zeros = torch.zeros(shape)\n",
    "print(\"torch_tensor_zeros:\\n {} \\n\".format(torch_tensor_zeros))\n",
    "torch_tensor_ones  = torch.ones(shape)\n",
    "print(\"torch_tensor_ones:\\n {} \\n\".format(torch_tensor_ones))\n",
    "torch_tensor_rand  = torch.rand(shape)\n",
    "print(\"torch_tensor_rand:\\n {} \\n\".format(torch_tensor_rand))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a164a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "torch_tensor_ones = torch.ones_like(torch_tensor)\n",
    "print(\"torch_tensor_ones:\\n {} \\n\".format(torch_tensor_ones))\n",
    "\n",
    "torch_tensor_zeros = torch.rand_like(torch_tensor_ones, dtype=torch.float) \n",
    "print(\"torch_tensor_rand:\\n {} \\n\".format(torch_tensor_rand))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cf994b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can access to the characteristics of a tensor such as:\n",
    "    \n",
    "* Its shape.\n",
    "* The type of the data it contains.\n",
    "* The device where it is held in memory (CPU/GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92873d23",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Shape:    {}\".format(torch_tensor_ones.shape))\n",
    "print(\"Datatype: {}\".format(torch_tensor_ones.dtype))\n",
    "print(\"Device:   {}\".format(torch_tensor_ones.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6558b0a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The access to the data is similar to numpy:\n",
    "\n",
    "* indices\n",
    "* **range** of indices: begin:end:step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b3c33a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "torch_tensor_ones[:,1] = -1\n",
    "print(torch_tensor_ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224ba888",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Two very useful operators:\n",
    "    \n",
    "* **concatenation** of tensors;\n",
    "* **reshaping** of a given tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cb4c30",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(torch_tensor_ones.shape,torch_tensor_rand.shape)\n",
    "torch_tensor_concatenated = torch.cat([torch_tensor_ones, torch_tensor_rand], dim=1) # Try 1,2\n",
    "print(torch_tensor_concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391b6d4d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(torch_tensor_concatenated.shape)\n",
    "torch_tensor_concatenated=torch_tensor_concatenated.reshape((2,10))\n",
    "print(torch_tensor_concatenated)\n",
    "print(torch_tensor_concatenated.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b9c3d3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the implementations you will play with, you will perform mathematical operations on tensors. There are many of them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2276b594",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Element-wise operations\n",
    "torch_tensor = torch.rand_like(torch_tensor_concatenated, dtype=torch.float) \n",
    "\n",
    "print(torch_tensor_concatenated*torch_tensor) \n",
    "print(torch_tensor_concatenated+torch_tensor)\n",
    "print(torch_tensor_concatenated-torch_tensor)\n",
    "print(torch_tensor_concatenated/torch_tensor)\n",
    "\n",
    "# Matrix multiplication\n",
    "print(torch_tensor.matmul(torch_tensor.T))\n",
    "print(torch_tensor @ torch_tensor.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6da3504",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Many functions apply some mathematical functions involving one or more dimensions in the tensors:\n",
    "    \n",
    "* mean\n",
    "* max\n",
    "* sum\n",
    "* cumsum\n",
    "\n",
    "For all of these, you need to specify the dimension(s) along which the operator applies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b2ea0a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(torch_tensor_concatenated)\n",
    "print(torch.mean(torch_tensor_concatenated)) # No axis applies the operation to all the elements\n",
    "print(torch.mean(torch_tensor_concatenated,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e419759",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Exercise**:\n",
    "\n",
    "* Generate a random $3\\times 1000 \\times 3$ tensor $\\mathbf T$ with values distributed as a uniform distribution between -10 and 10. \n",
    "* Produce a $3 \\times 3$ matrix $\\mathbf M$ deduced from $\\mathbf T$ by summing values along the second dimension.\n",
    "* Compute $\\mathbf M^3$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8537119",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand((3, 1000, 3)) * 20 - 10\n",
    "s = torch.sum(t, axis = 1)\n",
    "c = torch.pow(s, 3)\n",
    "print(s @ (s @ s))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7fa079",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Auto grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f0377c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The main interest in using libraries such as Pytorch, Tensorflow, JAX is to have a transparent implementation of the **computation of the gradients** of the user-defined loss function. \n",
    "\n",
    "All of these propose a mechanism of **auto-differentiation** (auto-grad), to perform this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3912438",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We recall that most supervised ML systems try to find a **parametrized funtion** $f_\\theta$, parameterized by some parameters $\\theta$,\n",
    "\n",
    "$$\n",
    "\\mathbf y = f_\\theta(\\mathbf x),\n",
    "$$\n",
    "\n",
    "being given a number of examples (a.k.a. the training data) $\\{\\mathbf x_i,\\mathbf y_i\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c266e64",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To adjust our model, we need to define a loss function $\\mathcal L()$ between the **expected output values** and our **model output values**:\n",
    "\n",
    "\n",
    "$$\n",
    "\\theta^* = \\arg\\min_\\theta \\sum_i \\mathcal L(\\mathbf y_i,f_\\theta(\\mathbf x_i)).\n",
    "$$\n",
    "\n",
    "\n",
    "For example, in regression problems:\n",
    "\n",
    "$$\n",
    "\\mathcal L(\\mathbf y,f_\\theta(\\mathbf x))=\\|\\mathbf y-f_\\theta(\\mathbf x)\\|^2.\n",
    "$$\n",
    "\n",
    "\n",
    "By using gradient descent:\n",
    "\n",
    "$$\n",
    "\\theta' = \\theta - \\alpha\\nabla_\\theta \\sum_i \\mathcal L(\\mathbf y_i,f_\\theta(\\mathbf x_i)).\n",
    "$$\n",
    "\n",
    "Auto-grad allows to compute this gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77497f19",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the DL jargon, this parameterized function is called a **model**.\n",
    "\n",
    "We will see later how to **build** our models; first, you have to know that Pytorch has a number of **predefined models**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5513cb2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch,torchvision\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "# A famous model which is predefined in Pytorch\n",
    "model= resnet18()\n",
    "\n",
    "# Dummy data: Note that there are 10 data: these is called a \"batch\"\n",
    "X      = torch.rand(10, 3, 64, 64) # This is a group of 10 random color images\n",
    "Y_true = torch.rand(10, 1000)      # We generate random target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a36d56",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# This is the forward pass:\n",
    "Y_pred = model(X)\n",
    "Y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9248ff9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "errors = (Y_pred - Y_true)**2 \n",
    "print(errors.shape) # Note the size\n",
    "\n",
    "# Sum all the squared differences\n",
    "loss = errors.sum()\n",
    "\n",
    "print(loss.shape, loss) # Note the size\n",
    "loss.backward() # backpropagation is done here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcffda53",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9) # model.parameters() is like theta - Stochastic gradient descent\n",
    "#lr is learning rate for deep learning; it's called step for machine learning\n",
    "optim.step() # apply one step of gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99391e35",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Setting up a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78296e15",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You define your model as a class inheriting from nn.Module.\n",
    "\n",
    "It needs:\n",
    "\n",
    "* a constructor (__init__ method)\n",
    "* a forward method, that implements the forwaard pass of your network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab9124a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Typically the constructor defines **instances** of the layers you will use in your network (convolutional, fully connected...).\n",
    "\n",
    "Examples:\n",
    "* nn.Linear\n",
    "* nn.Conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6258dd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then the **forward** method details how we apply these layers to our data.\n",
    "\n",
    "Often it follows a functional form:\n",
    "\n",
    "```\n",
    "x = f1(x)\n",
    "x = f2(x)\n",
    "x = f3(x)\n",
    "return x\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a0f41c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af066e06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A complete image classification example on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca621f67",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bbe939",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First step is to **prepare your data**.\n",
    "\n",
    "There are a number of builtin functions:\n",
    "\n",
    "* to load some very standard datasets (e.g., CIFAR10).\n",
    "* to encapsulate them into a **\"data loader\"** that allows us to cycle over the data **batch after batch** (e.g. in training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a01f8a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Form the training dataset\n",
    "trainset   = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader= torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Form the testing dataset\n",
    "testset    = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b98b282",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us see what these images look like with matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e3fac1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Functions to show an image\n",
    "def imshow(img):\n",
    "    img = img*0.5 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ea753d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Take the first batch and show the images\n",
    "dataiter       = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "print(images.shape, labels.shape)\n",
    "# Show images\n",
    "matplotlib.rcParams['figure.figsize'] = [14, 7]\n",
    "imshow(torchvision.utils.make_grid(images,nrow = 8))\n",
    "\n",
    "# Print their corresponding classes\n",
    "print(' '.join('{}'.format(classes[labels[j]]) for j in range(batch_size)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6acd074",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we will define a network to **perform classification**, i.e. attribute to each possible class (among the 10) a probablity.\n",
    "\n",
    "* Its **input** is a batch of 3x32x32 images.\n",
    "* Its **output** is a batch of 10x1 vectors.\n",
    "* We use convolutional layers to extract features from the image, at different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c04aee",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class imageClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 3 input image channel, 6 output channels, 5x5 square convolution\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # produce a batch of  6 x 14 x 14 features\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # produce a batch of 16 x 5 x  5 features\n",
    "        x = torch.flatten(x, 1)               # flatten to produce a batch of vectors\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "classifier = imageClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ce2adc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lossFunction = nn.CrossEntropyLoss()\n",
    "optimizer    = optim.Adam(classifier.parameters(), lr=0.0015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14df4bc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "# Each pass on this cycle will see all the batches from the training dataset\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(trainloader, 0): # enumerate(sequence, start = 0)\n",
    "        # Get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = batch\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass for the whole batch\n",
    "        outputs = classifier(inputs)\n",
    "        \n",
    "        # Evaluate the loss \n",
    "        loss = lossFunction(outputs, labels)\n",
    "        \n",
    "        # Apply backpropagation: compute the gradient of the loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Apply a gradient descent step: parameters = parameters - alpha * gradient\n",
    "        optimizer.step()\n",
    "\n",
    "        # Dislay loss values\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 ==  999:    # print every 1000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 1000:.3f}')\n",
    "            losses.append([epoch,running_loss / 1000.0])\n",
    "            running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9275b298",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "losses = np.array(losses)\n",
    "plt.plot(losses[:,0],losses[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62528dfb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "PATH = './classifier.pth'\n",
    "torch.save(classifier.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c20522",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "batch          = iter(testloader)\n",
    "images, labels = next(batch)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n",
    "_, predicted = torch.max(classifier(images), 1)\n",
    "print('Predicted:   ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
    "                              for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1852874e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for batch in testloader:\n",
    "        images, labels = batch\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(classifier(images).data, 1)\n",
    "        total   += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * correct // total}%')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6cbb4e84",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Exercise**:\n",
    "\n",
    "Recast the previous example as a ~~dog vs. non dog~~ animal vs non animal classification problem.    \n",
    "\n",
    "Use \n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bcba6e",
   "metadata": {},
   "source": [
    "# Image classifier model of two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddda6067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d489a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to show an image\n",
    "def imshow(img):\n",
    "    img = img*0.5 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d28a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformLabels(lbl):\n",
    "    animal = [2, 3, 4, 5, 6, 7]\n",
    "    return torch.Tensor([lbl in animal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30541a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Form the training dataset\n",
    "trainset   = torchvision.datasets.CIFAR10(root='../data/CIFAR10', train=True, download=True, transform=transform, target_transform = transformLabels)\n",
    "trainloader= torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Form the testing dataset\n",
    "testset    = torchvision.datasets.CIFAR10(root='../data/CIFAR10', train=False, download=True, transform=transform, target_transform = transformLabels)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "classes = (\"animaln't\", 'animal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50f7fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first batch and show the images\n",
    "dataiter       = iter(trainloader)\n",
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69cfdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show images\n",
    "matplotlib.rcParams['figure.figsize'] = [14, 7]\n",
    "imshow(torchvision.utils.make_grid(images,nrow = 8))\n",
    "# Print their corresponding classes\n",
    "print(' '.join('{}'.format(classes[int(labels[j])]) for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d212c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class animalClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.fc4 = nn.Linear(10, 1)\n",
    "        self.fc5 = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # produce a batch of  6 x 14 x 14 features\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # produce a batch of 16 x 5 x  5 features\n",
    "        x = torch.flatten(x, 1)               # flatten to produce a batch of vectors\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "classifier = animalClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b710339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lossFunction = nn.BCELoss()\n",
    "optimizer    = optim.Adam(classifier.parameters(), lr=0.0015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a037ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "# Each pass on this cycle will see all the batches from the training dataset\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(trainloader, 0):\n",
    "        # Get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = batch\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass for the whole batch\n",
    "        outputs = classifier(inputs)\n",
    "        # Evaluate the loss \n",
    "        loss = lossFunction(outputs, labels)\n",
    "        # Apply backpropagation: compute the gradient of the loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Apply a gradient descent step: parameters = parameters - alpha * gradient\n",
    "        optimizer.step()\n",
    "\n",
    "        # Dislay loss values\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 ==  999:    # print every 1000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 1000:.3f}')\n",
    "            losses.append([epoch,running_loss / 1000.0])\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = np.array(losses)\n",
    "plt.plot(losses[:,0],losses[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfad150",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch          = iter(testloader)\n",
    "images, labels = next(batch)\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join(f'{classes[int(labels[j])]:5s}' for j in range(batch_size)))\n",
    "_, predicted = torch.max(classifier(images), 1)\n",
    "print('Predicted:   ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
    "                              for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7341eee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for batch in testloader:\n",
    "        images, labels = batch\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        predicted = torch.where(classifier(images).data >= 0.5, 1, 0)\n",
    "        total   += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * correct // total}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27fcd3f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CPU/GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d836c3ca",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can check what devices your system has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa315ef",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Access to CUDA\n",
    "device    = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7bb13ce2",
   "metadata": {},
   "source": [
    "To use CUDA:\n",
    "\n",
    "* Use the **to** method to move the data/model to the GPU\n",
    "* Use the same method to get it back to CPU."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "pyTorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "a5894844b2d2ed0dfa0303dd3da765415d62adce6c74979413b2b103f5e23799"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
